{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efdb24cc",
   "metadata": {},
   "source": [
    "* Basics\n",
    "    * Use trained LLM to gen human-like text from given input prompt\n",
    "    * Sequential gen: Models predict and generate next tokein in a sequence, one word at a time\n",
    "    * Learned Probabilities: LLMs leverage probabilities from billions of parameters to generate coherent, Context relevant text\n",
    "* Role of Attention\n",
    "    * Attention Mechanism: Enables LLM to understand context & Generate coherent responses\n",
    "    * Relevant info: Focus on crucial words to predict next token (E.g. \"France\"&\"Capital\" in \"Capital of france is\")\n",
    "    * Advancements: Improve scaling & efficiency handling longer sequences\n",
    "        * Attention is what makes modern LLMs coherant and context aware.\n",
    "* Context Length & Attention Span\n",
    "    * Context Length: Max number of tokens, words, or parts of words an LLM can process @ once\n",
    "        * Working memory\n",
    "        * Limited by models architecture, size, available comp resources, complexity of input/output\n",
    "    * Limitations: Constrained by model architecture, computational resources, and input complexity\n",
    "    * Balancing Act: Different models are designed with varying context lengths to balance capability and efficiency\n",
    "* Art of prompting\n",
    "    * Prompting: Structuring input to guide LLM generation toward desired output\n",
    "        * Analyzing how LLM's predict the next token by analyzing input token importance helps craft better prompts\n",
    "    * Importance of wording: Input sequence's wording significantly influences model's predictions\n",
    "    * Guiding generation: Careful promt design helps steer the LLM's output\n",
    "* Two-Phase Inference Process\n",
    "    * Prefill Phase:\n",
    "        * Tokeization: Converting text to tokens\n",
    "        * Embedding Conversion: transforming tokens into numerical representations\n",
    "        * Initial Processing: running embeddings through the models networks\n",
    "            * This phase is computationally intensive, as it runs all input tokens at once.\n",
    "    * Decode Phase - Autoregressive text Generation one token at a time:\n",
    "        * Attention Computation: Looking back at previous tokens\n",
    "        * Probability Calculation: Liklihood of next token\n",
    "        * Token Selection: Choosing the next token\n",
    "        * Continuation Check: Deciding to stop or continue\n",
    "        * This phase is memory intensive as it tracks all previously generated tokens and their relationships\n",
    "* Sampling Strategies\n",
    "    * Control how model gens text, allowing adjustments b/w creativity and precision\n",
    "    * Token Selection: Begins with raw logits for all vocabulary words and transforms to token choices\n",
    "        * Process\n",
    "            1. Begins with Logits: Model's initial scores for each potential next word\n",
    "            2. Temperature control: A dial for creativity\n",
    "                * Higher values increase randomness, Lower make more focused & deterministic choices\n",
    "            3. Top-p sampling: Nucleaus sampling, considers only the most likely words whose probabilities sum up to a chosen threshold (e.g. 90%)\n",
    "            4. Top-k filtering: Considers only the top K most likely words\n",
    "    * Managing Repetition: Presence and frequency penalties to keep output fresh\n",
    "        * Penalties adjust raw probabilities early in the token selection process\n",
    "            * Presence Penalty: A fixed penalty applied to any token that's previously appeared, discouraging repetition of same words\n",
    "            * Frequency Penalty: A scaling penalty that increases with use of the same token, the more a word appears the less likelh it will be chosen again\n",
    "            * Goal: Nudge model to explore new vocab, keeping output fresh\n",
    "    * Controlling Generation Length: Control how much generated, like pacing a story.\n",
    "        * Token Limits: Min/Max counts\n",
    "        * Stop Sequences: Defining patterns to signal the end of generation (e.g. \"\\n\\n\" or \"win-win\")\n",
    "        * End-of-Sequence (EOS) Detection: Letting the model naturally conclude its response when it generates a special EOS token\n",
    "        * Example - Paragraph: Setting Max output of 100 tokens & using \"win-win\" as a stop sequence\n",
    "        * Goal: ensures focused & appropriately sized outputs\n",
    "    * Beam Search\n",
    "        * Hollistic Approach: explores multiple candidate sequences (beams) simultaneously\n",
    "        * Process:\n",
    "            * Maintain Candidate Sequences: Run several processes simultaneously\n",
    "            * Compute Probabilities: Compute the probabilities of each combination\n",
    "            * Select: Select sequence with highest overall probability\n",
    "            * Keep most promising combinations, beam width of sequences, and next tokens\n",
    "        * Benefit: More coherent, grammatically correct text\n",
    "        * Tradeoff: More computational resources for improved coherence\n",
    "    * Practical Challenges and Optimizations\n",
    "        * Key Metrics\n",
    "            * Time to First Token (TTFT): How quickly the first response appears, affected by prefill\n",
    "            * Time per Output Token (TPOT): Speed of generating subsequent tokens in the decode phase, O/A generation speed\n",
    "            * Throughput: How many requests handled simultaneously, affects scaling & cost\n",
    "            * **VRAM Usage: GPU memory usage, often the main constraint**\n",
    "        * Context Length Challenge: Offer more info, but increase memory usage quadratically and processing speed linearly. Balance is key.\n",
    "        * Key Value (KV) Cache Optimization: Stores & reuses calculations from the attention mechanism, speeding up inference (especially for long contexts) by reducing repeated computations. Increase memory usage, speed benefits usually outweigh costs\n",
    "* Conclusion\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf9626a",
   "metadata": {},
   "source": [
    "Questions:\n",
    "* Ask ChatGPT about Embedding conversion & Initial processing"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
