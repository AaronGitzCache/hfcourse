{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58529d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b54f32d3",
   "metadata": {},
   "source": [
    "# Transformers are Language models\n",
    "* Taught self supervised\n",
    "* Objective is automatically computed from the inputs of the model\n",
    "* Gains a statistical understanding of language it has been trained on, but less useful for practical tasks\n",
    "* General pretrained models go through transfer learning or fine tuning, where supervised training applied using human annotated labels.\n",
    "## Transfer Learning\n",
    "* Uses a model with a huge dataset to train a model with a small dataset to give the smaller model a different task\n",
    "* Training from scratch requires more data and compute to achieve comparable results\n",
    "* Training from scratch caps at ~68% accuracy, whereas fine-tuning pretrained models can reach 86 in 3 rounds (epochs?)\n",
    "* Pretraining is the act of training models from scratch, weights are randomly initialized and training starts w/o prior knowledge\n",
    "* Fine tuning is done after model pretrained using a dataset specific to your task\n",
    "    * Benefits are that the fine tuning process takes advantage of the knowledge acquired during PT, meaning it knows the basics of your language, etc.\n",
    "    * Less data more results\n",
    "    * Amt of time and resources needed to achieve results is much lower\n",
    "* Transfer learning removes the models head and replaces it while keeping its body.\n",
    "# Transformer Architecture\n",
    "* Encoders receive input & build representation of it (it's features), optimized to acquire understanding from the input\n",
    "    * Encoder only models are good for tasks requiring input understanding, such as sentence classification or NER\n",
    "* Decoders use the encoders representation along with other inputs to generate a target sequence, meaning the model is optimized for generating outputs\n",
    "    * Good for generative tasks\n",
    "* E-D models AKA sequence-to-sequence generate based on input, translation, summarization\n",
    "## Attention layers\n",
    "* Take for example English to Norwegian, translating \"I like the house\" or \"I like the cat\". The model can't just translate it directly without paying attention to the conjugation of Huset and Katten.\n",
    "    * Attention layers utilize context to affect the transformation of other words\n",
    "    * The encoder can use all of the words in the sentence, because it knows what is before & after\n",
    "    * The decoder only knows the last words it has spat out.\n",
    "# Architectures vs. checkpoints\n",
    "* Architectures: Skeleton of the model, definition of each layer and operation that happens w/i the model\n",
    "* Checkpoints: Weights loaded in a given architecture\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
