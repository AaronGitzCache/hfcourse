{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e197635f",
   "metadata": {},
   "source": [
    "# Quick Notes:\n",
    "If something's not working, restart the notebook using the restart button at the top.\n",
    "Make sure to use the .venv kernal to make sure it's working properly, you can use the code:\n",
    "\n",
    "```python\n",
    "import sys\n",
    "print(sys.executable)\n",
    "!pip list\n",
    "```\n",
    "If issues persist, delete the venv, create a new one, install requirements.txt and restart the kernel.\n",
    "## Locating models\n",
    "Models can be found in /Users/aaronfowler/.cache/huggingface\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6abc78",
   "metadata": {},
   "source": [
    "# NLP vs LLM\n",
    "* NLP: Natural Language Processing\n",
    "* LLM: Large Language Model\n",
    "* NLP is broader field focused on understanding/interpreting & generating human language, such as:\n",
    "* Sentiment analysis, entity recognition, machine translation\n",
    "* LLMs: Powerful subset of NLP w/ large size, extensive training, and wide range of language tasks w/ minimal task specific training\n",
    "## NLP and LLM Models\n",
    "* NLP: Field at intersection of ML and linguistics, understanding context not just words, sentiment analysis, text gen, speech recog\n",
    "## Rise of LLMS\n",
    "* Transformed NLP shifted from task specific models to general purpose\n",
    "LP is changing\n",
    "* Computers don't process like humans, language is complex: Ambiguity, context, nuance like sarcasm requires careful text representation\n",
    "## NLP Tasks\n",
    "* Classifying sentences: Sentiment analysis, Spam detection\n",
    "* Classifying words: Part of speech tagging, named entity recognition\n",
    "* Generating text: Auto-completion, translation, summarization\n",
    "* Extracting answers: Q&A based on context\n",
    "## Characteristics of LLMS\n",
    "* Scale: Mil-bil parameters\n",
    "* general capabilities w/o task specific training\n",
    "* learn from examples in context/in prompt\n",
    "* Demonstrate emergent abilities not explicitly programmed\n",
    "## Limitations of LLMS\n",
    "* Hallucination\n",
    "* lack of true understanding, opperating on stat patterns\n",
    "* Bias is reproduced\n",
    "* limited context windows\n",
    "* significant computational resources\n",
    "## Paradigm shift\n",
    "* Moved from specialized models to signle large model, made sophisticated LP more accesible, but challenges ethics, efficiency, deployment\n",
    "## Common NLP Tasks\n",
    "* Classifying whole sentences: Sentiment of a review, detect spam, determining grammar correctness\n",
    "* Classifying each word in a sentence: Noun, verb, adjective or named entities\n",
    "* Generate text content: Complete promt w/ autogen text, filling in blanks w/ masked word\n",
    "* Extracting answers based on context\n",
    "* Generating a new sentence from an input text\n",
    "* Not limited to written, can tackle speech recognition and computer vision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c32767",
   "metadata": {},
   "source": [
    "# TRANSFORMERS:\n",
    "* Transformers library provides functionality to create and use shared models\n",
    "* Most basic is the pipeline function\n",
    "    * Returns an end to end object that performs an NLP task on one or several texts\n",
    "    * Pre-processing -> Model -> Post-processing\n",
    "        * Pre-processing: Converts input to NLP readable format\n",
    "        * Model: Conducts computation of input\n",
    "        * Post-processing: Returns it in human readable output\n",
    "\n",
    "```python\n",
    "from transformers import pipeline # Load pipeline\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\") # Create a sentiment analysis classifier, which we've named classifier\n",
    "classifier(\"I love this course!\")# Test the classifier with a sample input\n",
    "```\n",
    "**OR** test with two inputs\n",
    "```python\n",
    "classifier = pipeline(\"sentiment-analysis\") # Create a sentiment analysis classifier\n",
    "# Test the classifier with a sample input\n",
    "classifier(\n",
    "    \"I love this course!\",\n",
    "    \"I can't get my finger out of my ass!\")\n",
    "# Responses\n",
    "```\n",
    "\n",
    "```bash\n",
    "[{'label': 'POSITIVE', 'score': 0.9998835325241089},\n",
    "{'label': 'NEGATIVE', 'score': 0.9990085959434509}]\n",
    "```\n",
    "It will label positive or negative, alongside a confidence score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60aefacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline # Load the sentiment analysis pipeline\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\") # Create a sentiment analysis classifier\n",
    "classifier([\"I love this course!\", \"My finger is stuck in my asshole!\"]) # Test with multiple inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54323144",
   "metadata": {},
   "source": [
    "## Zero-shot-classification\n",
    "* More generalized text classification pipeline\n",
    "* Allows you to provide labels you want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78396a85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to facebook/bart-large-mnli and revision d7645e1 (https://huggingface.co/facebook/bart-large-mnli).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'sequence': 'This School class is going to teach you loads of business shit',\n",
       " 'labels': ['business', 'education', 'politics'],\n",
       " 'scores': [0.7433503866195679, 0.2517228424549103, 0.00492677791044116]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "classifier = pipeline(\"zero-shot-classification\") # Create a zero-shot classifier\n",
    "classifier(\n",
    "    \"This School class is going to teach you loads of business shit\", # Input text to classify\n",
    "    candidate_labels=[\"education\", \"politics\", \"business\"]  #Specify the labels you want to classify against\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6682ec00",
   "metadata": {},
   "source": [
    "# Text Generation\n",
    "Text-generation pipeline uses an input prompt to generate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a390a43f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to openai-community/gpt2 and revision 607a30d (https://huggingface.co/openai-community/gpt2).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use mps:0\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Haven\\'t seen dad in months, he said he just needed some time to relax.\\n\\n\"I haven\\'t seen him in a while, so I\\'m just trying to enjoy life and stay away from things,\" said her mother, Karen.\\n\\n\"I don\\'t really know how I feel about it, but I know that mom will be there. She\\'ll be there for me, and she\\'ll be there for me if I\\'m not here for a long time.\"\\n\\nKaren said she has no idea why the two men ended up in a car that crashed into a tree in the driveway of her home.\\n\\nAfter the accident, Karen said she was able to call 911 and get help.\\n\\n\"I am just so grateful that our family and friends stopped by to help them, and to be so fortunate to be able to be here for them when they were a little bit older,\" said Karen.\\n\\n\"I\\'m so proud of them. I\\'m so proud of them because we really are thankful for this community and for this community.\"\\n\\nThe couple has been at the hospital since Wednesday after the incident.\\n\\nThe family of the car crash suspect is not believed to be in custody.\\n\\nThe family was not in the hospital at the time of the incident, but Karen said she is thankful for'}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(\"text-generation\") # Create a text generation pipeline\n",
    "generator(\"Haven't seen dad in months, he said he just needed some time to\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037b24d3",
   "metadata": {},
   "source": [
    "# Tasks\n",
    "Zero-Shot, Text-Generation & Sentiment-Analysis are all examples of 'Tasks', which you can use as a filter on HF when searching for [Models](https://huggingface.co/models).\n",
    "## Task examples:\n",
    "```python\n",
    "pipeline(\"text-classification\")\n",
    "pipeline(\"text-generation\")\n",
    "pipeline(\"summarization\")\n",
    "pipeline(\"translation\")\n",
    "pipeline(\"question-answering\")\n",
    "pipeline(\"zero-shot-classification\")\n",
    "pipeline(\"conversational\")\n",
    "pipeline(\"fill-mask\")\n",
    "pipeline(\"feature-extraction\")\n",
    "pipeline(\"token-classification\")\n",
    "```\n",
    "More tasks can be retreived using:\n",
    "```python\n",
    "from transformers.pipelines import SUPPORTED_TASKS\n",
    "print(SUPPORTED_TASKS.keys())\n",
    "```\n",
    "But tasks come after pipeline, so in the format:\n",
    "```python\n",
    "generator = pipeline(\"text-generation\")\n",
    "``` \n",
    "### The 'generator' is just naming the task in python so you know what it is.\n",
    "You can name it whatever the fuck you want ⬇️"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8ba026",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "wankbot = pipeline(\"text-generation\") # Create a text generation pipeline with a specific model\n",
    "wankbot(\"The pornography category i'm feeling like watching today is\", max_length=50, num_return_sequences=1) # Generate text with a maximum length and number of sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6314b9c0",
   "metadata": {},
   "source": [
    "# Choosing Models\n",
    "After starting off with our standard\n",
    "```python\n",
    "from transformers import pipeline\n",
    "```\n",
    "We can select our model by specifying it after our pipeline type:\n",
    "\n",
    "```python\n",
    "generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "```\n",
    "#Additional Arguments\n",
    "We can specify additional arguments such as maximum length, and the number of times we want to run the sequence (how many times it will run the task). For example:\n",
    "```python\n",
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(\"text-generation\", model=\"distilgpt2\")\n",
    "generator(\n",
    "    \"When I get sad, I like to\",\n",
    "    max_length=30,\n",
    "    num_return_sequences=3,\n",
    ")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6cf059",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(\"text-generation\", model=\"distilgpt2\")\n",
    "generator(\n",
    "    \"When I get sad, I like to\",\n",
    "    truncation=True,\n",
    "    max_length=30,\n",
    "    num_return_sequences=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df7cdec",
   "metadata": {},
   "source": [
    "# Unmasking\n",
    "Predicts missing words in the sentence. Note that different models may not use ```<mask>``` as their mask token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca546add",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "unmasker = pipeline(\"fill-mask\") # Create a fill-mask pipeline\n",
    "unmasker(\"My dad left us, he said he was going to get <mask> from the store.\", top_k=3) #Top-k=3 means it will return the top 3 predictions for the masked token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9967dc",
   "metadata": {},
   "source": [
    "# Classifying Text With Named Entity Recognition (NER)\n",
    "In this example, the model has to find parts of input text corresponding to entities such as people, orgs, or location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c678490c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "ner = pipeline(\"ner\", grouped_entities=True) # Create a named entity recognition pipeline with grouped entities\n",
    "ner(\"My name is Aaron and I study at Lund University in Sweden.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08cb9cfd",
   "metadata": {},
   "source": [
    "## Formula\n",
    "When we pass the argument 'Grouped_entities=True' in pipeline function, we tell PL to regroup together parts of the sentence that correspond to the same entity. You can see this in the response for Lund University, where it grouped those two words into the same entity. Without grouping the entities, the response will return each token seperately, which could be problematic as may times models use sub-word tokenization.\n",
    "## Response\n",
    "```bash\n",
    "[{'entity_group': 'PER',\n",
    "  'score': np.float32(0.9987024),\n",
    "  'word': 'Aaron',\n",
    "  'start': 11,\n",
    "  'end': 16},\n",
    " {'entity_group': 'ORG',\n",
    "  'score': np.float32(0.98618424),\n",
    "  'word': 'Lund University',\n",
    "  'start': 32,\n",
    "  'end': 47},\n",
    " {'entity_group': 'LOC',\n",
    "  'score': np.float32(0.99971896),\n",
    "  'word': 'Sweden',\n",
    "  'start': 51,\n",
    "  'end': 57}]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c00235c",
   "metadata": {},
   "source": [
    "# Questions answering\n",
    "The questions-answering pippeline answers questions using info from a given context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d88ac17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "qna = pipeline(\"question-answering\") # Create a question-answering pipeline\n",
    "qna(\n",
    "    question=\"What is my name?\",\n",
    "    context=\"My name is Aaron and I study at Lund University in Sweden.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb38793",
   "metadata": {},
   "source": [
    "## Response\n",
    "The response **doesn't generate the answer**, it simply extracts the information.\n",
    "```Bash\n",
    "{'score': 0.9961819648742676, 'start': 11, 'end': 16, 'answer': 'Aaron'}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc38195",
   "metadata": {},
   "source": [
    "# Summarization\n",
    "Reducing text into shorter text while keeping all or most of important aspects referenced in the text. You can still specify min/max length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc80ca6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "summarizer = pipeline(\"summarization\") # Create a summarization pipeline\n",
    "summarizer(\n",
    "    \"My name is Aaron and I study at Lund University in Sweden. I am currently taking a course on natural language processing, which is really interesting. The course covers various topics such as sentiment analysis, text generation, and named entity recognition.\",\n",
    "    max_length=10,\n",
    "    min_length=5,\n",
    "    do_sample=False\n",
    ") # Summarize the input text with specified length constraints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc34f7a",
   "metadata": {},
   "source": [
    "# Translation\n",
    "You can use a default model if you provide a language pair in the task name and use models from Helsinki-NLP's opus-mt Collections. E.g.\n",
    "```python\n",
    "from transformers import pipeline\n",
    "\n",
    "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-fr-en\")\n",
    "translator(\"Ce cours est produit par Hugging Face.\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf20be4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-fr-en\")\n",
    "translator(\"Ce cours est produit par Hugging Face.\") # Translate French to English"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
