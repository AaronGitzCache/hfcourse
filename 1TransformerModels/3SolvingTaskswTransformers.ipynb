{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6923480",
   "metadata": {},
   "source": [
    "# Transformer models for language\n",
    "* Understand and generate human language by learning patterns and relationships b/w words or tokens in text\n",
    "* Originally designed for machine translation, now the default architecture for solving AI tasks\n",
    "* Some tasks are better for encoder structure, while others best suited for decoder, and some both\n",
    "## How LMs work\n",
    "* Learn stat patterns and relationships between words\n",
    "* Trained to predict the probability of a word given the context of surrounding words, giving them a foundational understanding of language that can generalize to other tasks\n",
    "* Two Main approaches for training a transformer model\n",
    "    * Masked Language Modeling (MLM): Used by encoders like BERT, randomly masks some tokens in the input and trains the model to predict the original tokens based on surrounding context.\n",
    "        * Teaches the model bi-directional context: Looking at words both before & after masked word\n",
    "    * Casual Language Modeling (CLM)\n",
    "        * Used by decoder models like GPT, predicts next token based on all previous tokens in a sequence. Can only look backwords, reviewing previous text as context to predict next tokens\n",
    "## Types of LMs\n",
    "* Econder only (BERT)\n",
    "    * Use bidirectional approach to understand context from both sides, best suited for deep understanding of text such as classifications, named entity recognition, and Q&A\n",
    "    * Understand/Classify Text\n",
    "        * Assign predefined categories to text\n",
    "        * Uses wordpiece tokenization and special tokens like SEP and CLS to differentiate sentences and capture representation\n",
    "        * Token Classification assigns labels to each token in a sequence (like NER)\n",
    "            * BERT can be adapted for this using a linear layer that adds a label to each token for prediction\n",
    "        * Q&A Finds answers w/i given context\n",
    "        * Bert with span classification head\n",
    "            * Predicts start and end positions of the answers in the text\n",
    "* Decoder only (GPT, Llama)\n",
    "    * Process text from left to right and are great for generation\n",
    "    * Generate text\n",
    "        * Create coherent and context relevant text based on a prompt\n",
    "        * Uses byte pair encodings (BPE) & positional encodings makes it highly effective\n",
    "* Encoder-Decoder (T5, BART)\n",
    "    * Both approaches - encoder to understand the input and decoder to generate output.\n",
    "    * Excel at sequence to sequence tasks like translation, summarization, and Q*A.\n",
    "    * Summarization Condese long text into short text while preserving key info\n",
    "        * BART & T5 En/Decoder models\n",
    "            * Pretrained with text infilling corruption, teaching the model to predict missing tokens\n",
    "    * Translation Converting text from one language to another\n",
    "        * Adapts to translation by using separate encoder to map the source language to the target language\n",
    "* Text Generation\n",
    "    * Creating coherent & contextually relevent text based on a prompt or input\n",
    "        * E.g. GPT2, decoder only model pretrained on large amount of text, generating convincing but not always true text given a prompt & complete other NLP tasks like QA despite not being explicitly trained to\n",
    "        * ![GPT2](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/gpt2_architecture.png)\n",
    "        * Step 1\n",
    "            * Uses byte pair encoding (BPE) to tokenize words and generate a token embedding.\n",
    "            * Positional encodings are added to the token embeddings that indicate the position of each token in a sentence\n",
    "            * W/i each encoder block, GPT2 uses masked self-attention layer, meaning it can't attend to future tokens (only using tokens on the left)\n",
    "                * Differs from BERT's mask token because, in masked self0attention, an attention mask is used to set the score to 0 for future tokens\n",
    "        * Step 2\n",
    "            * Output from decoder is passed to a LM head, performing linear trans to convert the hidden states into logits (logits are a probability function that maps probability values (from 0-1) to real numbers (-inf. to +inf.))\n",
    "* Text Classification \n",
    "    * Assigning predefined categories to text documents, such as sentiment analysis, topic classification, or spam detection\n",
    "    * Bert is an encoder-only model, first to implement deep bidirectionality to learn richer representations of text by attending to words on both sides\n",
    "        * Step 1:\n",
    "            * Bert uses [wordpiece](https://huggingface.co/docs/transformers/tokenizer_summary#wordpiece) tokenization\n",
    "                * Wordpiece tries to maximize the liklihood that the next letter merges with the previous to create the most likely signal pair\n",
    "            * To tell the difference between 1 and multiple sentences, a special [SEP] token differentiates them into sequences\n",
    "            * CLS token is added to the beginning of every sequence in the text\n",
    "            * Final output with CLS token is used as the input to the classification head for tasks\n",
    "            * Also adds segment embeddings to denote whether a token belongs in first or second sentence in a pair of sentences\n",
    "        * Step 2:\n",
    "            * BERT is pretrained w/ two objectives\n",
    "                * 1. Masked language modeling\n",
    "                    * Some percentage of input tokens are randomly masked, model predicts these\n",
    "                        * solves bidirectional issue, where model could cheat and see all words and \"predict\" the next words\n",
    "                    * Final hidden states of predicted mask tokens are passed to a feedforward network to predict the next word\n",
    "                * 2. Next-sentence prediction: predicting whether sentence B follows sentence A\n",
    "                    * Half of the time Sentence B is the next sentence, other half sentence B is a random sentence. Predicts whether it's the next sentence or not, and is passed to feedforward network over two classes (IsNext and NotNext)\n",
    "        * Step 3: Input embeddings passed through multiple encoder layers to output some final hidden states\n",
    "        * To use the pretrained model for TC, it needs a sequence classification head on top of the base BERT model\n",
    "            * Sequence class. head is a linear layer accepting final hidden states & performing linear transformation to convert them into logits. Cross entropy loss is calculated b/w logits and target to find most likely label\n",
    "        * **ATTENTION** Look into the [<Text Classification Course>](https://huggingface.co/docs/transformers/tokenizer_summary#wordpiece) As it may help AIC\n",
    "* Token Classification\n",
    "    * Assigning label to each token in a sequence, such as in NER or part-of-speech-tagging\n",
    "    * Using BERT for Token Class. like NER:\n",
    "        * Add a Tok. Clas. head on top of base BERT model\n",
    "            * TC Head is a linear layer that accepts final states and performs linear transformation to convert them into logits\n",
    "                * Cross-entropy loss is calculated b/w logits and each token to find most likely label\n",
    "* Questions Answering\n",
    "    * Using BERT:\n",
    "        * Add \"Span Classification\" Head on Base BERT model.\n",
    "            * Linear head accepting final hidden states & performing linear transformation to compute the span start and end logits corresponding to answer\n",
    "            * Cross-entropy loss calculated b/w logits and label positions to find most likely span of text corresponding to answer\n",
    "* Summarization\n",
    "    * Long text to short version while preserving key info and meaning\n",
    "    * En/Decoder models like BART and T5 use sequence to sequence (S2S) patterns of summarization.\n",
    "    * How BART works\n",
    "        * Encoder architecture similar to BERT, accepting tokens & positional embedding of text\n",
    "        * BART pretrained by corrupting input and restructuring it w/ decoder\n",
    "            * Text infilling corruption: number of text spans replaced w/ a single mask token, teaching model to predict # of tokens\n",
    "        * Input embeddings & masked spans passed through encoder to output some final hidden states\n",
    "            * Unlike BERT, BART doesn't add final feedforward network at the end to predict a word\n",
    "        * Enconder output passed to detector: Predicts masked tokens and uncorrupted tokens from output, helping gives additional context to help decoder restore original text.\n",
    "        * Decoder output passed to LM head, performing linear transformation to convert hidden states to logits. Cross-entropy loss calced b/w logits & label, which is token shifted to right\n",
    "* Translation\n",
    "    * S2S task converting text from one language to another while preserving meaning\n",
    "    * BART translates by adding separate randomly initialized encoder to map source language to map source language to an input that can be decoded in the target language. New encoder embeddings are passed to pretrained encoder instead of original word embeddings.\n",
    "    * source encoder trained by updating source encoder, posit. embeddings, input embeddings w/ cross-entropy loss from model output.\n",
    "    * Parameters frozen in first step, and trained together in second step.\n",
    "## Modalities beyond text\n",
    "* Speech and audio\n",
    "    * Automatic speach recognition (ASR)\n",
    "    * Whisper: En/decoder model pretrained on massive dataset (680k hours) of labeled audio data enabling zero-shot performance across many languages & tasks. \n",
    "        * Decoder allows Whisper to map encoders learned speech representations to useful outputs like text, w/o additional fine tuning.\n",
    "        * ![Whisper](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/whisper_architecture.png)\n",
    "        * 2 Main components\n",
    "            * Encoder processes input audio, raw audio converted into log-mel spectrogram which is passed through transformer encoder network\n",
    "            * Decoder takes encoded audio representation and autoregressively predicts corresponding text toens.\n",
    "                * Standard transformer decoder trained to predict the next token given the previous tokens and encoder output\n",
    "                * Special tokens are used at the beginning of decoder input to steer model towards specific tasks like transcription, translation, or language identification.\n",
    "* Computer Vision - Image classification\n",
    "    * Vision transformer (ViT)\n",
    "        * Splits images into non-overlapping patches, each of which is turned into a vector or patch embedding\n",
    "            * Patch embeddings are generated from a convolutional 2D layer which creates the proper input dimensions\n",
    "            * A 224x224 image is split into 196 16x16 image patches, just like how text is tokenized into words, an image is tokenized into a sequence of patches\n",
    "        * A learnable embedding - CLS tokens added at beginning of patch embeddings (like bert) for classification to capture overall image representation\n",
    "            * CLS tokens are classification tokens, that classify objects and their representations\n",
    "        * The final hidden state of CLS token used as input to attached classification head, other outputs ignored, helping model learn to encode representations of an image\n",
    "        * Positional embeddings added as the model doesn't know how patches are ordered. These are also learnable and have same size as patch embeddings.\n",
    "        * All embeddings passed through transformer\n",
    "        * Uses attention mechanisms\n",
    "        * Position embeddings map the positions of image tokens to order patches before passing them through the encoder\n",
    "    * ConvNeXT - Modern Convolutional Neural Network (CNN) used for image classifcation\n",
    "        * Transformer like designs\n",
    "        * Uses convolutions\n",
    "        * Used for image classification\n",
    "        * Combines the strength of conventional layers with modern network designs\n",
    "    * Object detection, segmentation, and depth estimation\n",
    "## LLM Course Documentation"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
