{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbb3af2a",
   "metadata": {},
   "source": [
    "# 3 Main Architectual Variants of Transformer Models\n",
    "## Encoder\n",
    "* Outputs a numerical representation [Like a sequence of numbers] for **each word** used as input. This numerical vector can also be referred to as a feature vector or feature tensor\n",
    "* Each vector is a numerical representation of the word in question\n",
    "    * Dimension of vector is defined by the architecture of model\n",
    "* Representations contain the value of the word but contextualized, meaning it's representation is influenced bidirectionally\n",
    "    * Words on either side are called context - welcome **to** NYC::Left Context=Welcome & Right=NYC\n",
    "* Does this with the self attention mechanism\n",
    "    * Relates to different positions or words in a single sequence to compute a representation\n",
    "    * Resulting representation of a word affected by other words in sequence\n",
    "### When to Use an Encoder\n",
    "* When Bi-directional context is necessary to capture meaningful info about a sequence\n",
    "* Extracting meaningful information\n",
    "    * Masked Language Modeling (MLM): Guessing a randomly masked word \"My <mask> is Aaron\"\n",
    "    * Sentiment Analysis: Analyze sentiment of a sequence\n",
    "        * Encoders are good at obtaining an understanding of sequences; and the relationship/interdependence between words\n",
    "* NLU: Natural Language Understanding\n",
    "## Decoders\n",
    "* Attention layers can only access words positioned before it in a sentence, AKA auto-regressive, meaning they predict next word.\n",
    "* Word -> Decoder -> Numerical representation for each comprised of the word & the last words (masked self attention)\n",
    "    * W/o right context - \"Welcome **TO** NYC\", To vector is just **Welcome -> TO** excluding NYC\n",
    "* Causal Language Modeling: Mapping the most probable following word\n",
    "    * My -> Name\n",
    "    * My Name -> Is #Auto-regressive meaning that the model reuses its past output in the following steps\n",
    "    * My Name Is Aaron -> . #This is where context size comes in\n",
    "    * Starting from a single word, we've generated a full sentence. We can continue to the max context size.\n",
    "        * If Max Context size is 1024, it can generate 1024 words while retaining memory of the first word generated.\n",
    "* Models like Gemma, Llama, Deepseek belong to this family\n",
    "### Modern LLMs\n",
    "* Use **ONLY Decoder architecture**\n",
    "* Trained in 2 phases\n",
    "    * Pretraining: Model learns to predict next token on vast amounts of text data\n",
    "    * Instruction tuning: Model is fine-tuned to follow instructions & generate helpful responses\n",
    "* Models can understand and generate human-like text across a wide range of topics and tasks, allowing them to:\n",
    "\n",
    "| Capability         | Description                              | Example                                   |\n",
    "|--------------------|------------------------------------------|-------------------------------------------|\n",
    "| Text Gen           | Create coherent and context-relevant text| Write essays, stories, emails             |\n",
    "| Summarize          | Consolidate long docs to short           | Create executive summary of report        |\n",
    "| Translate          | Converting text between languages        | \"You get it\"                              |\n",
    "| Question Answering | Provide answers to factual questions     | \"What is the capital of France?\"          |\n",
    "| Code Generation    | Write or complete code snippets          | Create an app                             |\n",
    "| Reasoning          | Work through problems step by step       | Solve logical puzzles                     |\n",
    "| Few-Shot Learning  | Learn from a few examples in the prompt  | Classify text after seeing 2â€“3 examples   |\n",
    "# Sequence-to-Sequence Models\n",
    "* Use both architectures\n",
    "* attention layers of encoder can access all words in the initial sentence, whereas decoder can only access words positioned before a given word in the input\n",
    "* Pretraining\n",
    "    * Often involves reconstructing sentence for which the input has been corrupted (masked random words, etc.)\n",
    "* Practical apps, Machine translation, Text summarization, Data-to-text generation, Grammar correction, Questions Answering\n",
    "# Attention Mechs\n",
    "* Most models use full attention in the sense that the attention matrix is square, bottlenecking long texts\n",
    "* longformer & reformer models speed up training efficiency using sparse vision of attention matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8a57c2",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
