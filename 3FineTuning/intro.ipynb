{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbdadde7",
   "metadata": {},
   "source": [
    "This chapter covers:\n",
    "* preping large dataset from the hub using HF dataset features\n",
    "* Using high-level trainer API to fine-tune a model with modern best practices\n",
    "* Implement custom training loop with optimization techniques\n",
    "* Leverage HF accelerate library to run distributed trianing on any setup\n",
    "* How to apply current fine-tuning best practices for maximum performance* \n",
    "* Additionally, this chapter covers other libraries such as datasets, tokenizers, accelerate, and evaluate to train models more efficiently & effectively\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
