{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f40544b",
   "metadata": {},
   "source": [
    "# Tokenizers\n",
    "* In NLP most data is raw text, but ML can't read or understand in raw form\n",
    "    * Only Work with numbers\n",
    "* Tokenizer translates text to numbers\n",
    "* Several approaches - Objective is to find most meaningful representation:\n",
    "## Word-Based\n",
    "* ![Image](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/word_based_tokenization.svg)\n",
    "* There're different ways to split text. For example, using whitespace to tokenize text into words by applying python's split function:\n",
    "```python\n",
    "tokenized_text = \"Jim Henson was a puppeteer\".split()\n",
    "print(tokenized_text)\n",
    "```\n",
    "Would output:\n",
    "```['Jim', 'Henson', 'was', 'a', 'puppeteer']```\n",
    "* Variations of WTs w/ extra punctuation rules.\n",
    "    * Can have large \"vocabularies\", w/ vocab defined by total number of independent tokens we have in our corpus.\n",
    "    * Each word assigned ID, from 0-(vocabsize). Model uses IDs to indentify each word\n",
    "    * The problem:\n",
    "        * There're >500k words in English language, lots of tokens\n",
    "        * Dog & Dogs are similar, but model wouldn't be able to relate them\n",
    "        * We need custom token to represent words not in vocab, known as \"unknown\" token (often represented as \"UNK\" or \"<unk>\")\n",
    "        * Goal is to have **AS FEW WORDS AS POSSIBLE UNK**\n",
    "        * Which is why we use a character based tokenizer\n",
    "## Character-Based\n",
    "* CB splits chars rather than words, w/ two main benifits:\n",
    "    * Smaller vocab\n",
    "    * Fewer out-of-vocab tokens since you can build all words\n",
    "* Also not perfect: Problems w/ spaces & punctuation\n",
    "    * Less meaningful than words, but language dependent\n",
    "        * In Chinese, each character carries more info than latin language\n",
    "    * We'll end up w/ very large amt of tokenz to be processed by model\n",
    "        * WB = 1 Token, CB = 10 Tokens\n",
    "        * Thus, Subword\n",
    "## Subword-Based\n",
    "* Freq. used words not split, but rare words should be\n",
    "* E.g. \"Annoyingly\" is rare, thus, ```[\"Annoying\", \"ly\"]```, as both will show as frequent standalone subwords, while the meaning is kept by the composite meaning of annoyingly.\n",
    "* E.g. \"Tokenization\"\n",
    "    * BERT writes ```['token','##ization'] where \"##\" shows that it's PART of the word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08caa03a",
   "metadata": {},
   "source": [
    "## Loading & Saving\n",
    "* ```from_pretrained()``` & ```save_pretrained()``` will load or save alrgorithm used by tokenizer (like the architecture of the model) and it's vocab (weights of the model)\n",
    "* Loading BERT tokenizer trained w/ same checkpoint as BERT is done same way as loading model but w/ ```BertTokenizer``` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a967a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fe2f16",
   "metadata": {},
   "source": [
    "* Like ```AutoModel```,```AutoTokenizer``` class grabs proper tokenizer class in lib based on checkpoint name, & can be used w/ any checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2003da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2440479",
   "metadata": {},
   "source": [
    "* We can now use tokenizer as shown in previous section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d84a6b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 7993, 170, 13809, 23763, 2443, 1110, 3014, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"Using a Transformer network is simple\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9fbed18",
   "metadata": {},
   "source": [
    "* Saving Tokenizer is same as saving model:\n",
    "```python\n",
    "tokenizer.save_pretrained(\"directory_on_my_computer\")\n",
    "```\n",
    "* How input_ids are generated looking @ intermediate methods of tokenizer:\n",
    "## Encoding & Decoding\n",
    "### Encoding\n",
    "* Translating text -> Numbers = encoding\n",
    "* Two steps:\n",
    "    * Tokenization\n",
    "        * Split text in to (words, sub-word, character)\n",
    "        * Must match pretraining of model\n",
    "    * Conversion to Input IDs\n",
    "        * Convert tokens to numbers, so we can build a tensor out of them & feed the model\n",
    "        * Tokenizer has vocabulary which is downloaded when we instantiate ```from_pretrained()```\n",
    "            * Must be same vocab as when model was pretrained\n",
    "#### Tokenization\n",
    "***This demo does the two steps seperately to show outputs, but in practice you should call tokenizer directly on your inputs***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6e97b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "sequence = \"Using a Transformer network is simple\"\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96de944",
   "metadata": {},
   "source": [
    "Returns ```['Using', 'a', 'Trans', '##former', 'network', 'is', 'simple']```\n",
    "* This is sub-word tokenizer, note ```'Trans','##former'``` splitting the uncommon word\n",
    "* Raw Text -> Tokens :: \"Let's try to tokenize!\" -> ```[let,',s,try,to,token,##ize,!]```\n",
    "    * May Lowercase all words, then split everything into small text chunks\n",
    "    * Most use SW tok algorithm\n",
    "* Tokens + Special Tokens :: ```[[CLS],let,',s,try,to,token,##ize,!,[SEP]]```\n",
    "    * ```##``` is indication used by BERT to denote that token isn't the beginning of a word\n",
    "        * Others may be different, like ALBERT using ```_``` to all tokens w/ space before them\n",
    "* T+ST -> Input IDs :: ```[101, 2292, 1005, 3046, 2000, 19204, 4697, 999, 102]```\n",
    "    * Maps token IDs to vocab of tokenizer\n",
    "    * Must match to make sure we use same as models pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcafe47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8692d37a",
   "metadata": {},
   "source": [
    "Returns ```[7993, 170, 13809, 23763, 2443, 1110, 3014]```\n",
    "* the conversion to IDs is handled by ```covert_tokens_to_ids()``` method\n",
    "* Once converted to appropriate framework tensor, these inputs sent to model\n",
    "## Decoding\n",
    "* Turns vocab indicies -> string w/ the ```decode()``` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624c0c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_string = tokenizer.decode([7993, 170, 11303, 1200, 2443, 1110, 3014])\n",
    "print(decoded_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af7a8d8",
   "metadata": {},
   "source": [
    "Returns: ```Using a transformer network is simple```\n",
    "* Decode not only converts, but groups SW tokens to produce readable sentence\n",
    "* Useful w/ models predicting new text (either generated or translated/summarized)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
