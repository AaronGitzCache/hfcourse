{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f40544b",
   "metadata": {},
   "source": [
    "# Tokenizers\n",
    "* In NLP most data is raw text, but ML can't read or understand in raw form\n",
    "    * Only Work with numbers\n",
    "* Tokenizer translates text to numbers\n",
    "* Several approaches - Objective is to find most meaningful representation:\n",
    "## Word-Based\n",
    "* ![Image](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/word_based_tokenization.svg)\n",
    "* There're different ways to split text. For example, using whitespace to tokenize text into words by applying python's split function:\n",
    "```python\n",
    "tokenized_text = \"Jim Henson was a puppeteer\".split()\n",
    "print(tokenized_text)\n",
    "```\n",
    "Would output:\n",
    "```['Jim', 'Henson', 'was', 'a', 'puppeteer']```\n",
    "* Variations of WTs w/ extra punctuation rules.\n",
    "    * Can have large \"vocabularies\", w/ vocab defined by total number of independent tokens we have in our corpus.\n",
    "    * Each word assigned ID, from 0-(vocabsize). Model uses IDs to indentify each word\n",
    "    * The problem:\n",
    "        * There're >500k words in English language, lots of tokens\n",
    "        * Dog & Dogs are similar, but model wouldn't be able to relate them\n",
    "        * We need custom token to represent words not in vocab, known as \"unknown\" token (often represented as \"UNK\" or \"<unk>\")\n",
    "        * Goal is to have **AS FEW WORDS AS POSSIBLE UNK**\n",
    "        * Which is why we use a character based tokenizer\n",
    "## Character-Based\n",
    "* CB splits chars rather than words, w/ two main benifits:\n",
    "    * Smaller vocab\n",
    "    * Fewer out-of-vocab tokens since you can build all words\n",
    "* Also not perfect: Problems w/ spaces & punctuation\n",
    "    * Less meaningful than words, but language dependent\n",
    "        * In Chinese, each character carries more info than latin language\n",
    "    * We'll end up w/ very large amt of tokenz to be processed by model\n",
    "        * WB = 1 Token, CB = 10 Tokens\n",
    "        * Thus, Subword\n",
    "## Subword-Based\n",
    "* Freq. used words not split, but rare words should be\n",
    "* E.g. \"Annoyingly\" is rare, thus, ```[\"Annoying\", \"ly\"]```, as both will show as frequent standalone subwords, while the meaning is kept by the composite meaning of annoyingly.\n",
    "* E.g. \"Tokenization\"\n",
    "    * BERT writes ```['token','##ization'] where \"##\" shows that it's PART of the word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08caa03a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
