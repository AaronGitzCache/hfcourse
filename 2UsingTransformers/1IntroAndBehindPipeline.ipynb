{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11a785e3",
   "metadata": {},
   "source": [
    "# Intro\n",
    "* HFT - Hugging face transformers\n",
    "* API - Application Programming Interface\n",
    "* HFT Models large, millions to 10s of billions params, training and deploying is complicated\n",
    "* HFT library was created to provide a signle API for loading, training, saving\n",
    "    * Ease of use - DL, load & use NLP models for inference can be done w/ two lines of code\n",
    "    * Flexibility - all models are simple pytorch (nn.Module) or TensorFlow (tf.keras.Model) classes, handled same as other models in respective ML frameworks\n",
    "    * Simplicity - All in one file so the code is understandable & hackable\n",
    "* HFT different from other libraries\n",
    "    * Other ML libs built on modules shared across files\n",
    "    * HFT models have their own layers, allowing you to experiment w/ one model w/o affecting other models\n",
    "* Chapter Contents\n",
    "    * Replicate end to end example using model & tokenizer to replicate the pipeline() function\n",
    "    * Discuss model API\n",
    "        * Model & Config classes\n",
    "        * Load model showing how it processes numerican inputs to output predictions\n",
    "    * Tokenizer API - Main component of Pipeline function\n",
    "        * First & Last processing steps, handling:\n",
    "            * conversion from text to numerical inputs for neural network\n",
    "            * Conversion back to text when needed\n",
    "    * Batching multiple sentences through model\n",
    "    * High-level tokenizer() function\n",
    "# Behind the Pipeline\n",
    "**Reminder**\n",
    "![Image](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/full_nlp_pipeline.svg)\n",
    "* Tokenization: Raw Text to Input IDs\n",
    "    * Raw text inputed \"This course is amazing!\"\n",
    "    * Split into tokens [This, course, is, amazing, !]\n",
    "    * Model adds special tokens [[CLS], this, course, is, amazing, !, [SEP]]\n",
    "        * CLS stands for \"classify\", placed at beginning of input sequence for sequence classification or next sentence prediction\n",
    "            * Summary token absorbing contents of sentence\n",
    "        * SEP is \"separator\" used to separate two segments or sentences in the same input or mark the end of a sentence/segment\n",
    "    * Input ID: Matches each token to it's unique ID to the vocabulary of the pretrained model\n",
    "        * [101, 2023, 2607, 2003, 6429, 999, 102]\n",
    "## Chapter 1\n",
    "The code:\n",
    "```Python\n",
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "classifier(\n",
    "    [\n",
    "        \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "        \"I hate this so much!\",\n",
    "    ]\n",
    ")\n",
    "```\n",
    "Obtained:\n",
    "```bash\n",
    "[{'label': 'POSITIVE', 'score': 0.9598047137260437},\n",
    " {'label': 'NEGATIVE', 'score': 0.9994558095932007}]\n",
    " ```\n",
    "### Preprocessing W/ Tokenizer\n",
    "* HFT Models can't process raw text, thus converts inputs to numbers using tokenizer to:\n",
    "    * Split input into words, subwords, symbols called tokens\n",
    "    * Map each to integer\n",
    "    * Add additional inputs that may be useful to the model\n",
    "* All of this needs to match the way the model was pretrained\n",
    "    * That info can be downloaded from model hub\n",
    "    * Use ```AutoTokenizer``` Class & its ```from_pretrained()``` method.\n",
    "    * Use model checkpoint name, auto fetches data associated w/ mode's tokenizer & cache it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "220ecacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer # Note that AutoTokenizer is case-sensitive\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e4e8bb",
   "metadata": {},
   "source": [
    "* First time will download tokenizer\n",
    "* Once we have the tokenizer, we can pass our sentence to it & get a dictionary to be fed to our model\n",
    "    * Don't worry about ML framework used as backend when using HFT, may be PyTorch or TensorFlow or Flax\n",
    "    * TF models only accept tensors as inputs\n",
    "        * Tensors are like NumPy arrays, could be scalar (0d), vector(2d), or matrix (3d)\n",
    "* To specify tensor type (PyTorch, TensorFlow, or plain NumPy) we use ```return_tensors``` argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c1db526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n",
      "          2607,  2026,  2878,  2166,  1012,   102],\n",
      "        [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer # Note that AutoTokenizer is case-sensitive\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "raw_inputs = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "    \"I hate this so much!\",\n",
    "]\n",
    "inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef9482f",
   "metadata": {},
   "source": [
    "Response:\n",
    "```Bash\n",
    "{'input_ids': tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172, 2607,  2026,  2878,  2166,  1012,   102],\n",
    "        [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,     0, 0,     0,     0,     0,     0,     0]]),\n",
    "        'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]])}\n",
    "```\n",
    "* The attention mask details where padding was applied, so that the model pays it no attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0def8597",
   "metadata": {},
   "source": [
    "### Going Through the model\n",
    "* HFT provides an AutoModel class which also has a from_pretrained() method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04064597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 16, 768])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel # AutoModel only instantiates the BODY of the model, not the head\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = AutoModel.from_pretrained(checkpoint)\n",
    "outputs = model(**inputs)\n",
    "print(outputs.last_hidden_state.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a700b7",
   "metadata": {},
   "source": [
    "* Downloaded or ensured presence of checkpoint used in pipeline before\n",
    "* this architecture contains only base transformer module: given some inputs, it outputs hidden states (aka features)\n",
    "* For each model input, output a high-dimensional vector representing the **contextual understanding of that input by the TF model**\n",
    "    * These hidden states are inputs to another part of the model, the head\n",
    "### High Dimensional Vector\n",
    "* Vector output by the TF module is usually large w/ 3 dimensions:\n",
    "    * **Batch Size:** Number of sequences processed at a time (2 in our example)\n",
    "    * **Sequence Length:** Length of numerical representation of the sequence\n",
    "        * Details how many tokens in each sequence **After tokenization & padding**\n",
    "        * Transformers need **fixed-length inputs**, so they should always have fixed lengths\n",
    "        * shorter sentences are padded (e.g. \"I love NLP\")\n",
    "            * Simplified tokenization [I, love, NLP] -> Padded [I, love, NLP, Pad, Pad, Pad]\n",
    "        * longer are truncated (E.g.\"This sentence is way too long and must be cut\")\n",
    "            * Tokenized [this, sentence, is, way, too, long, and, must, be, cut] -> [This, sentence, is, way, too, long]\n",
    "    * **Hidden Size:** Vector dimension of each model input\n",
    "        * Size of each individual token embedding/vector\n",
    "        * Defines richness of models internal representation of each token\n",
    "        * E.g. in bert-base-uncased, hidden size 768, meaning each token is a 768 dimensional vector after processing\n",
    "        * Larger models can reach 3072 or more\n",
    "    * **Overall Example**\n",
    "        * If using bert-base-uncased, model output shape is:\n",
    "        ```torch.Size([2,16,768])```\n",
    "        * We can see this by running:\n",
    "        ```python\n",
    "        outputs = model(**inputs)\n",
    "        print(outputs.last_hidden_state.shape)\n",
    "        ```\n",
    "* Outputs of HFT models behave like namedtuples or dictionaries\n",
    "* access elements by:\n",
    "    * attributes (like python code above)\n",
    "    * key ```(outputs[\"last_hidden_state\"])```\n",
    "    * index (if you  know exactly where the thing you're looking for is) ```(outputs[0])```.\n",
    "### Model Heads: Making Sense of Numbers\n",
    "* Take high-dim vectors of hidden states as input & project them onto a different dimension\n",
    "* Output sent to model head for processing\n",
    "* Composed of one or few linear layers:\n",
    "![Heads](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/transformer_and_head.svg)\n",
    "* In diagram:\n",
    "    * Model represented by embeddings layer & subsequent layers\n",
    "        * Embeddings converts each input ID in the tokenized input into a vector representing associated token\n",
    "        * Subsequent layers manipulate those vectors using attention mechanisms to produce final representation of sentences\n",
    "* Many architectures in HFT, each one designed around a specific task. Some are:\n",
    "    * Model (retrieve the hidden states)\n",
    "        * E.g. BertModel or GPT2Model\n",
    "        * Only returns hidden states - raw internal representations from transformer layers\n",
    "            * Input words➡️Converted to numbers (tokens)➡️Processed through Transformer Layers➡️Outputs a vector (long list of numbers) for each token at each layer\n",
    "            * Output vectors are called hidden states, can be thought of like model's internal understanding of each word in the sentence - thought cloud containing meaning, grammar, context\n",
    "        * Useful if you want to build something custom, only provides base model\n",
    "    * ForCausalLM\n",
    "        * Used for text generation in GPT style models to predict next word\n",
    "    * ForMaskedLM\n",
    "        * Fill-in-the-blank to predict missing words\n",
    "    * ForMultipleChoice\n",
    "        * Choose best answer out of several options\n",
    "    * ForQuestionAnswering\n",
    "        * Extract Answer span - given context & question, finds start and end position of answer inside context\n",
    "    * ForSequenceClassification\n",
    "        * Sentiment or intent\n",
    "    * ForTokenClassification\n",
    "        * Tag each words, like NER to find names\n",
    "* EX: Using sequence classification head, we use AutoModelForSequenceClassification instead of just AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "de0ed663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "outputs = model(**inputs)\n",
    "print(outputs.logits.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55fb91f1",
   "metadata": {},
   "source": [
    "* Output is ```torch.Size([2, 2])```\n",
    "    * There are 2 characters as this is a 2D tensor (matrix) with\n",
    "        * 2 Rows - one for each input sentence\n",
    "        * 2 Columns - One for each class label\n",
    "    * In general shape is: [batch_size, num_labels]\n",
    "* When looking at output shape, dimensionality will be much lower\n",
    "    * Model takes high-dimensional vectors as inputs\n",
    "    * Outputs vectors containing two values (one per label)\n",
    "    * Since we have just two sentences, result we get is 2x2\n",
    "### Post Processing Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f6ba149a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.5607,  1.6123],\n",
      "        [ 4.1692, -3.3464]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(outputs.logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4237ee5c",
   "metadata": {},
   "source": [
    "* Our model predicted [-1.5607, 1.6123] for the first sentence & [ 4.1692, -3.3464] for the second\n",
    "    * These aren't probabilities but **Logits**\n",
    "        * Raw, unnormalized scores outputted by the last layer of the model\n",
    "        * To be converted to probabilities, they need to go through a SoftMax layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9dbdc1cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.0195e-02, 9.5981e-01],\n",
      "        [9.9946e-01, 5.4419e-04]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30de7c82",
   "metadata": {},
   "source": [
    "* **Probability Scores:** Model predicted [4.0195e-02, 9.5981e-01] for first & [9.9946e-01, 5.4419e-04] for second\n",
    "* To get labels corresponding to each position, we can inspect ```id2label``` attribute of model config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8377e199",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'NEGATIVE', 1: 'POSITIVE'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.id2label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057a2a0d",
   "metadata": {},
   "source": [
    "```{0: 'NEGATIVE', 1: 'POSITIVE'}```\n",
    "* Now we can conclude that the first sentence:\n",
    "    * Negative: 0.0402, Positive: 0.9598\n",
    "* Second sentence:\n",
    "    *  Negative: 0.9995, Positive: 0.0005"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
