{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "966da15e",
   "metadata": {},
   "source": [
    "# Batching Input Sequences Together\n",
    "## Model Expects Batch Inputs\n",
    "Convert list of numbers to a tensor & send it to model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a7922e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
    "\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "input_ids = torch.tensor(ids)\n",
    "# This line will fail.\n",
    "model(input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1f6703",
   "metadata": {},
   "source": [
    "Result: ```IndexError: Dimension out of range (expected to be in range of [-1, 0], but got 1)```\n",
    "### Why did it fail?\n",
    "* Sent single sequence to the model\n",
    "    * HFT models expect multi sentence by default\n",
    "    * The tokenizer added a dimension on top\n",
    "### Trying again w/ new dimension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb47037",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
    "\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "input_ids = torch.tensor([ids])\n",
    "print(\"Input IDs:\", input_ids)\n",
    "\n",
    "output = model(input_ids)\n",
    "print(\"Logits:\", output.logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa687b1",
   "metadata": {},
   "source": [
    "* Result is input IDs & resulting logits:\n",
    "```bash\n",
    "Input IDs: [[ 1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,  2607, 2026,  2878,  2166,  1012]]\n",
    "Logits: [[-2.7276,  2.8789]]\n",
    "```\n",
    "* Batching is the act of sending multiple sentences through the model, w/ only 1 sentence, you can batch w/ a single sequence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4515e3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_ids = [ids, ids]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e233127",
   "metadata": {},
   "source": [
    "* Creates a batch of two identical sequences\n",
    "* Batching let's model work when you feed it multi sequences\n",
    "* Issue: When batching 2+ sentences, they may be diff lengths\n",
    "    * Tensors require rectangular shapes, preventing conversion of list inputIDs to tensor directly\n",
    "    * Solution: ```pad``` input\n",
    "* Can't be converted to tensor:\n",
    "```python\n",
    "batched_ids = [\n",
    "    [200, 200, 200],\n",
    "    [200, 200]\n",
    "]\n",
    "```\n",
    "* To work around, padding makes rectangular shape, ensuring all sentences have same length by adding special word called ***padding token*** to the sentence w/ fewer values.\n",
    "    * E.g. if you have 10 sentences w/ 10 words & 1 w/ 20, padding ensures all have 20.\n",
    "```python\n",
    "padding_id = 100\n",
    "\n",
    "batched_ids = [\n",
    "    [200, 200, 200],\n",
    "    [200, 200, padding_id],\n",
    "]\n",
    "```\n",
    "* Padding token ID can be found in ```tokenizer.pad_token_id```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ecb02949",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.5694, -1.3895]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 0.5803, -0.4125]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 1.5694, -1.3895],\n",
      "        [ 1.3374, -1.2163]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "sequence1_ids = [[200, 200, 200]]\n",
    "sequence2_ids = [[200, 200]]\n",
    "batched_ids = [\n",
    "    [200, 200, 200],\n",
    "    [200, 200, tokenizer.pad_token_id],\n",
    "]\n",
    "\n",
    "print(model(torch.tensor(sequence1_ids)).logits)\n",
    "print(model(torch.tensor(sequence2_ids)).logits)\n",
    "print(model(torch.tensor(batched_ids)).logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ffd07f",
   "metadata": {},
   "source": [
    "Returns:\n",
    "```bash\n",
    "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n",
    "tensor([[ 1.5694, -1.3895]], grad_fn=<AddmmBackward0>)\n",
    "tensor([[ 0.5803, -0.4125]], grad_fn=<AddmmBackward0>)\n",
    "tensor([[ 1.5694, -1.3895],\n",
    "        [ 1.3374, -1.2163]], grad_fn=<AddmmBackward0>)\n",
    "```\n",
    "* Problem: Logits prediction should be the same for the second sentence, but we have different values\n",
    "    * This is bc attention layers contextualize each token\n",
    "        * Take into account the padding tokens, applying to all tokens in sentence\n",
    "    * Solution:to get same result when passing indiv. sentences of diff lengths through the model or passing batch w/ same sentences & padding applied, we need to tell attention layers to ignore padding tokens using **attention mask**\n",
    "### Attention masks\n",
    "* Tensors w/ exact same shape as input IDs tensor, filled w/ 0s & 1s.\n",
    "    * 1s indicate corresponding tokens should be attended to\n",
    "    * 0s indicate tokens shouldn't be attended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc4d3383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.5694, -1.3895],\n",
      "        [ 0.5803, -0.4125]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "batched_ids = [\n",
    "    [200, 200, 200],\n",
    "    [200, 200, tokenizer.pad_token_id],\n",
    "]\n",
    "\n",
    "attention_mask = [\n",
    "    [1, 1, 1],\n",
    "    [1, 1, 0],\n",
    "]\n",
    "\n",
    "outputs = model(torch.tensor(batched_ids), attention_mask=torch.tensor(attention_mask))\n",
    "print(outputs.logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8cf6bd0",
   "metadata": {},
   "source": [
    "Result:\n",
    "```Bash\n",
    "tensor([[ 1.5694, -1.3895],\n",
    "        [ 0.5803, -0.4125]], grad_fn=<AddmmBackward0>)\n",
    "```\n",
    "* Now ew get the same logits for the second sentence in batch bc last value of second sequence is a padding ID w/ - value in attention mask\n",
    "\n",
    "# Longer sequences\n",
    "* W/ HFT models, limit lengths for sequences. Most handle sequences of < 512 or 1024 tokens but will crash if longer. Two solutions\n",
    "    * Use a model w/ longer supported sequence length\n",
    "        * Models have different supported sequence lengths, some specialize in handling v long sequences.\n",
    "            * Longformer & LED are what you need to look into..\n",
    "    * Truncate sequences\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
